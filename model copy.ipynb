{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dfc85b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spearmanr \u001b[38;5;66;03m# Para a estabilidade SHAP\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m \u001b[38;5;66;03m# Certifique-se de que a biblioteca SHAP está instalada\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# from aif360.metrics import ClassificationMetric # Ignorando por enquanto, usando cálculo manual de EOD\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Define o nome da coluna sensível para uso consistente\u001b[39;00m\n\u001b[32m     11\u001b[39m SENSITIVE_FEATURE_NAME = \u001b[33m'\u001b[39m\u001b[33mQ\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from scipy.stats import spearmanr # Para a estabilidade SHAP\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap # Certifique-se de que a biblioteca SHAP está instalada\n",
    "# from aif360.metrics import ClassificationMetric # Ignorando por enquanto, usando cálculo manual de EOD\n",
    "\n",
    "# Define o nome da coluna sensível para uso consistente\n",
    "SENSITIVE_FEATURE_NAME = 'Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee05732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_combine_data(directory_path):\n",
    "    X_train = pd.read_csv(f\"{directory_path}/X_train.csv\")\n",
    "    y_train = pd.read_csv(f\"{directory_path}/y_train.csv\", header=None, names=['Y'])\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    X_test = pd.read_csv(f\"{directory_path}/X_test.csv\")\n",
    "    y_test = pd.read_csv(f\"{directory_path}/y_test.csv\", header=None, names=['Y'])\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "    \n",
    "    # Assumimos que a primeira coluna de X_train é a variável sensível Q\n",
    "    # (O Bias on Demand geralmente a nomeia como 'Q' ou um proxy)\n",
    "    # Precisamos confirmar o nome da coluna Q quando gerarmos os dados\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51db755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_data, test_data, baseline_feature_ranking=None):\n",
    "    \"\"\"Treina o modelo, calcula métricas e estabilidade SHAP.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Separar Features (X) e Target (y)\n",
    "    X_train = train_data.drop('Y', axis=1)\n",
    "    y_train = train_data['Y']\n",
    "    X_test = test_data.drop('Y', axis=1)\n",
    "    y_test = test_data['Y']\n",
    "\n",
    "    # --- 1. Model Training ---\n",
    "    model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # --- 2. Robustness Metrics ---\n",
    "    results['Overall_Accuracy'] = accuracy_score(y_test, y_pred)\n",
    "    results['Overall_F1_Score'] = f1_score(y_test, y_pred)\n",
    "    results['AUROC'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # --- 3. Fairness Metrics (Equal Opportunity Difference - EOD) ---\n",
    "    \n",
    "    # Identificar a coluna Q e os grupos\n",
    "    Q_test = X_test[SENSITIVE_FEATURE_NAME]\n",
    "    \n",
    "    # Cálculo do True Positive Rate (TPR)\n",
    "    # TPR = Recall = TP / (TP + FN) = Proporção de verdadeiros positivos\n",
    "    \n",
    "    # Grupo Privilegiado (Q=0)\n",
    "    TP_P = ((y_test == 1) & (Q_test == 0) & (y_pred == 1)).sum()\n",
    "    P_pos = ((y_test == 1) & (Q_test == 0)).sum()\n",
    "    TPR_P = TP_P / P_pos if P_pos > 0 else 0\n",
    "    \n",
    "    # Grupo Desprivilegiado (Q=1)\n",
    "    TP_U = ((y_test == 1) & (Q_test == 1) & (y_pred == 1)).sum()\n",
    "    U_pos = ((y_test == 1) & (Q_test == 1)).sum()\n",
    "    TPR_U = TP_U / U_pos if U_pos > 0 else 0\n",
    "\n",
    "    results['Equal_Opportunity_Difference'] = TPR_P - TPR_U\n",
    "    results['Disparate_Impact_Ratio'] = (y_pred[Q_test == 1].mean() / \n",
    "                                        y_pred[Q_test == 0].mean()) if y_pred[Q_test == 0].mean() > 0 else 0\n",
    "\n",
    "    # --- 4. Explainability Metrics (SHAP Stability) ---\n",
    "    explainer = shap.Explainer(model, X_train)\n",
    "    # Calcule os valores SHAP apenas para uma amostra para velocidade\n",
    "    shap_values = explainer(X_test.sample(n=min(500, len(X_test)), random_state=42)) \n",
    "    \n",
    "    # Média Absoluta SHAP por feature (Feature Importance)\n",
    "    mean_abs_shap = pd.Series(np.abs(shap_values.values).mean(axis=0), index=X_test.columns)\n",
    "    \n",
    "    # Armazenar o ranking atual para comparação (ordenado pelos valores)\n",
    "    current_feature_ranking = mean_abs_shap.sort_values(ascending=False).rank(method='first')\n",
    "\n",
    "    results['Feature_Importance_Ranking'] = current_feature_ranking.to_dict()\n",
    "\n",
    "    if baseline_feature_ranking is not None:\n",
    "        # Calcular a correlação de posto de Spearman contra a linha de base\n",
    "        # Usamos apenas as features comuns e na ordem correta\n",
    "        \n",
    "        # Converter para Series para garantir a ordem das features\n",
    "        I_base = pd.Series(baseline_feature_ranking)\n",
    "        I_current = current_feature_ranking\n",
    "        \n",
    "        # Alinhar os índices (nomes das features)\n",
    "        common_features = I_base.index.intersection(I_current.index)\n",
    "        \n",
    "        # Spearman Rank Correlation\n",
    "        rho, _ = spearmanr(I_base.loc[common_features].values, \n",
    "                           I_current.loc[common_features].values)\n",
    "        results['SHAP_Rank_Stability'] = rho\n",
    "    else:\n",
    "        results['SHAP_Rank_Stability'] = 1.0 # Linha de base é 100% estável contra si mesma\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ffd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de diretórios de experimentos (substitua pelos seus caminhos reais)\n",
    "experiment_directories = [\n",
    "    '/path/to/baseline',\n",
    "    # Bias Series (l_q)\n",
    "    '/path/to/bias_lq_0.5', \n",
    "    '/path/to/bias_lq_0.7',\n",
    "    # Noise Series (sy)\n",
    "    '/path/to/noise_sy_0.1',\n",
    "    '/path/to/noise_sy_0.2',\n",
    "    # Imbalance Series (p_u)\n",
    "    '/path/to/imbalance_pu_0.6',\n",
    "    '/path/to/imbalance_pu_0.2'\n",
    "]\n",
    "\n",
    "master_results_df = pd.DataFrame()\n",
    "baseline_ranking = None\n",
    "\n",
    "print(\"--- Iniciando Loop de Auditoria ---\")\n",
    "\n",
    "for i, dir_path in enumerate(experiment_directories):\n",
    "    print(f\"\\nProcessando: {dir_path.split('/')[-1]}\")\n",
    "    \n",
    "    train_data, test_data = load_and_combine_data(dir_path)\n",
    "    \n",
    "    if train_data is None:\n",
    "        continue # Pular se houver erro de carregamento\n",
    "\n",
    "    # Determinar se é a linha de base\n",
    "    is_baseline = (i == 0)\n",
    "\n",
    "    # Executar o experimento\n",
    "    current_results = run_experiment(train_data, test_data, \n",
    "                                     baseline_feature_ranking=baseline_ranking)\n",
    "    \n",
    "    # Se for a Linha de Base, capturar o ranking para uso futuro\n",
    "    if is_baseline:\n",
    "        baseline_ranking = current_results['Feature_Importance_Ranking']\n",
    "        print(\"-> Linha de Base estabelecida.\")\n",
    "\n",
    "    # Adicionar metadados ao resultado\n",
    "    current_results['Condition'] = 'Baseline' if is_baseline else dir_path.split('/')[-1].split('_')[0]\n",
    "    current_results['Parameter_Value'] = 'N/A' if is_baseline else dir_path.split('_')[-1]\n",
    "    \n",
    "    # Converter para DataFrame de uma linha e anexar ao mestre\n",
    "    results_row = pd.DataFrame([current_results])\n",
    "    master_results_df = pd.concat([master_results_df, results_row], ignore_index=True)\n",
    "    \n",
    "    print(f\"   -> EOD: {current_results['Equal_Opportunity_Difference']:.3f} | F1: {current_results['Overall_F1_Score']:.3f} | Estabilidade SHAP: {current_results['SHAP_Rank_Stability']:.3f}\")\n",
    "\n",
    "# Remover o ranking SHAP complexo para simplificar o arquivo final\n",
    "master_results_df = master_results_df.drop(columns=['Feature_Importance_Ranking'], errors='ignore')\n",
    "\n",
    "# Salvar a tabela final\n",
    "MASTER_RESULTS_FILE = 'project_audit_results.csv'\n",
    "master_results_df.to_csv(MASTER_RESULTS_FILE, index=False)\n",
    "print(\"\\n--- Loop Concluído ---\")\n",
    "print(f\"Resultados salvos em: {MASTER_RESULTS_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
