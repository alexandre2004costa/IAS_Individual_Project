Combine bias w missing data ...
In bias datasets, apply SMOT and compara w baseline model


#Bias and fairness
Two harms:
    - Allocation: The system extends or witholds opportunities, resources, or information.
    - Quality-of-Service: The system does not work equally well for all groups.

Stereotyping: The system reinforces stereotypes.

Sources of Bias: Historical Bias
‚Ä¢ Pre-existing bias reflected in the data, such as representational harms (e.g., stereotypes). This
means that the data collected from the world as it is (or was) can still inflict harm on a
population.

Sources of Bias: Representation Bias
‚Ä¢ Occurs when certain parts of the input space are underrepresented: the data is not
representative of the target population, contains underrepresented subgrouos, the sampling
method used cannot capture the populations characteristics.

Sources of Bias: Measurement Bias
‚Ä¢ Collecting data requires design:
‚Ä¢ Do features adequately capture complex notions needed for the decision making? Should those notions be
used? Is the outcome a well-defined measurement or a biased proxy that reinforces existing inequality? Is
that outcome worth modeling?
‚Ä¢ Measurement bias occurs when choosing, collecting, or computing features and labels to
use in a prediction problem, related to the problem of proxies

Sources of Bias: Aggregation Bias
‚Ä¢ Occurs when a ‚Äùone-fits-all‚Äù model is used in data where there are distinct groups that
should be treated differently, leading to the creation of a model that is not adequate for any
groups or that fits only the dominant population.

Sources of Bias: Learning Bias
‚Ä¢ Related to certain model choices that may amplify
disparities when building a model, such as prioritizing
overall accuracy over disparate impact. Fairness always
comes at a cost and we need to define suitable tradeoffs for it (fairness-accuracy, fairness-privacy, ‚Ä¶).

Sources of Bias: Evaluation Bias
‚Ä¢ Arises when misrepresentative benchmarks are used to test and compare models. If these
benchmarks are biased, then the benchmark itself encourages the development of methods that
only perform well on the subset of data represented in the benchmark.

Sources of Bias: Deployment Bias
‚Ä¢ Occurs when there is a mismatch between the problem a model is intended to solve and the
way the model is actually used. Systems do not operate outside from society and human bias


PROTECTED VARIABLES CAN BE USED TO MEASURE FAIRNESS IN MACHINE LEARNING MODELS

What is Fairness?
‚Ä¢ ‚ÄúFairness is the absence of any prejudice or favoritism towards an individual or group based
on their intrinsic or acquired traits in the context of decision-making.‚Äù

Individual Fairness: Similar individuals should receive similar predictions.

Group Fairness: Groups defined by splitting the population by protected attributes (e.g., race, gender ) should be treated equally.

‚Ä¢ Equality of Outcome: The outcome distribution across groups should be the same (e.g., same success rate for Black, Hispanic, White, and Asian candidates in a hiring system).
‚Ä¢ Equality of Opportunity: Different groups should be given the same opportunity (e.g., individuals who qualify for a positive outcome should be treated equally regardless of their ethnicity.

Equality of Outcome strives to achieve an equal likelihood of positive predictive outcome (success rate). For instance, success rates should be equal for ‚Äúmale‚Äù and ‚Äúfemale‚Äù groups.

Based on the equality of outcome (equal success rates), we can define two popular fairness metrics: Disparate Impact Ratio and Statistical Parity.
‚Ä¢ The Disparate Impact Ratio (DIR) quantifies the deviation from equality based on demographic parity as:
    - DIR = P(C=Ci|G=g1) / P(C=Ci|G=g2) = SRg1 / Srg2
    DIR ranges from [0,‚àû], where a value of 1 is ideal, indicating demographic parity.

‚Ä¢ Statistical Parity (SP) considers the difference in success rates rather than a ratio and is defined as:
    - SP = P(C=Ci|G=g1) - P(C=Ci|G=g2) = SRg1 - Srg2
    Ideally, SP should approximate 0

‚Ä¢ Rather than looking globally at the success rates, the Equality of Opportunity aims to ensure that
    individuals that qualify for a positive outcome are treated similarly regardless of their
    membership to a particular demographic group, which requires the True Positive (TPR) to be
    equal across groups.
    P(Yp=1|G=g1, Y=y) = P(Yp=1|G=g2, Y=y),y=1
    Yp is the predicted output and Y is the actual outcome

‚Ä¢ Derived from the notion of equal opportunity is the Equalized Odds metrics, which measures
    whether a given prediction is independent of the group of a sensitive attribute:
    P(Yp=1|G=g1, Y=y) = P(Yp=1|G=g2, Y=y),y ‚àà {0,1}
    Note that now y ‚àà {0,1}. Therefore, P(ùëåp =1|G=g1,Y=y) will be the TPRg1 if y=1 and FPRg1 if y=0.

‚Ä¢ The Average Odds Difference (AOD) quantifies the Equality of Odds:
    AOD = 0.5[(FRPg1-FRPg2) + (TRPg1-TRPg2)]
    It measures the average of the differences between the FPR and the TPR between groups.
    Ideally, it should approximate 0, indicating an identical performance between groups.